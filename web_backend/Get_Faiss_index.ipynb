{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5fcc581b",
      "metadata": {
        "id": "5fcc581b"
      },
      "source": [
        "# Hướng dẫn truy vấn dữ liệu thị giác dùng fiftyone\n",
        "\n",
        "Đây là hướng dẫn dùng cho các đội tham dự AI Challenge 2023. Hướng dẫn này nhằm mục đích giới thiệu cho các đội một phương pháp cơ bản để truy vấn dữ liệu dựa trên thông tin BTC cung cấp và giới thiệu công cụ fiftyone để hỗ trợ đội thi đánh giá kết quả.\n",
        "\n",
        "## Cài đặt ban đầu\n",
        "\n",
        "Bạn cần cài đặt môi trường để chạy được notebook này trên máy tính cá nhân của bạn. Hướng dẫn này không bao gồm phần cài đặt môi trường. Khuyến nghị: các bạn có thể cài đặt [Anaconda](https://docs.anaconda.com/free/anaconda/install/windows/).\n",
        "\n",
        "## Cài đặt các thư viện FiftyOne và PyTorch\n",
        "Hướng dẫn này dùng fiftyone là công cụ để trực quan dữ liệu và pytorch là backend chính cho các thuật toán máy học.\n",
        "\n",
        "### Lưu ý: Đối với các bạn dùng Windows nên dùng bản fiftyone **v0.21.4**, không nên dùng bản mới nhất!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a576d0",
      "metadata": {
        "id": "f5a576d0"
      },
      "outputs": [],
      "source": [
        "! pip install fiftyone==0.21.4\n",
        "! pip install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9afb40",
      "metadata": {
        "id": "fb9afb40"
      },
      "source": [
        "Load dữ liệu keyframe từ thư mục chứa keyframe. Mỗi ảnh và thông tin đi kèm sau này sẽ được lưu trữ trong một Sample. Tất cả các Sample được lưu trong Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "357d4489",
      "metadata": {
        "id": "357d4489"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.brain as fob\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import json\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b7ba43",
      "metadata": {
        "id": "36b7ba43"
      },
      "source": [
        "Load dữ liệu keyframe từ thư mục chứa keyframe. Trong hướng dẫn này tất cả các file Keyframes_L*.zip được giải nén vào thư mục `D:\\AIC\\Keyframes`. Mỗi ảnh và thông tin đi kèm sau này sẽ được lưu trữ trong một `Sample`. Tất cả các `Sample` được lưu trong `Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e072688",
      "metadata": {
        "id": "9e072688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |█████████████| 67619/67619 [9.3s elapsed, 0s remaining, 5.9K samples/s]       \n"
          ]
        }
      ],
      "source": [
        "dataset = fo.Dataset.from_images_dir('D:\\WorkSpace\\Contest\\HCM_AIC2023\\dataset\\keyframes', name=None, tags=None, recursive=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b89e71c",
      "metadata": {
        "id": "9b89e71c"
      },
      "source": [
        "Sau khi dữ liệu đã load lên xong. Bạn có thể truy cập vào đường vào ứng dụng web của fiftyone từ [http://localhost:5151](http://localhost:5151)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "078ce744",
      "metadata": {
        "id": "078ce744"
      },
      "source": [
        "Hoặc bạn có thể chạy cell bên dưới để mở tab mới cho ứng dụng web fiftyone"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28298924",
      "metadata": {
        "id": "28298924"
      },
      "source": [
        "### Trích xuất thêm thông tin tên của video và frameid\n",
        "Thông tin `video` và `frameid` sẽ được lấy từ tên của tập tin keyframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a8ee02ad",
      "metadata": {
        "id": "a8ee02ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sample in dataset:\n",
        "    _, sample['video'], sample['frameid'] = sample['filepath'][:-4].rsplit('\\\\', 2)\n",
        "    sample.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d66008",
      "metadata": {
        "id": "11d66008"
      },
      "source": [
        "Bạn có thể xem `Sample` đầu tiên của `Dataset` bằng lệnh sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6997ef1c",
      "metadata": {
        "id": "6997ef1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Sample: {\n",
            "    'id': '65017bf58c34b7c265c46da7',\n",
            "    'media_type': 'image',\n",
            "    'filepath': 'D:\\\\WorkSpace\\\\Contest\\\\HCM_AIC2023\\\\dataset\\\\keyframes\\\\L01_V001\\\\0001.jpg',\n",
            "    'tags': [],\n",
            "    'metadata': None,\n",
            "    'video': 'L01_V001',\n",
            "    'frameid': '0001',\n",
            "}>\n"
          ]
        }
      ],
      "source": [
        "print(dataset.first())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4309a34f",
      "metadata": {
        "id": "4309a34f"
      },
      "source": [
        "### Thêm thông tin kết quả của object detection.\n",
        "\n",
        "Bước này có thể tốn của bạn nhiều thời gian để đọc hết tất cả các dữ liệu về object detection. Bạn có thể bỏ qua cell này và chạy cell này sau nếu muốn thử thêm các thông tin về vector CLIP embedding trước."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ad1c4395",
      "metadata": {
        "id": "ad1c4395"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'D:\\\\AIC\\\\objects\\\\L01_V001\\\\0001.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m dataset:\n\u001b[0;32m      2\u001b[0m     object_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mAIC\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mobjects\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m{\u001b[39;00msample[\u001b[39m'\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39m{\u001b[39;00msample[\u001b[39m'\u001b[39m\u001b[39mframeid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(object_path) \u001b[39mas\u001b[39;00m jsonfile:\n\u001b[0;32m      4\u001b[0m         det_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(jsonfile)\n\u001b[0;32m      5\u001b[0m     detections \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\AIC\\\\objects\\\\L01_V001\\\\0001.json'"
          ]
        }
      ],
      "source": [
        "for sample in dataset:\n",
        "    object_path = f\"D:\\\\AIC\\\\objects\\\\{sample['video']}\\\\{sample['frameid']}.json\"\n",
        "    with open(object_path) as jsonfile:\n",
        "        det_data = json.load(jsonfile)\n",
        "    detections = []\n",
        "    for cls, box, score in zip(det_data['detection_class_entities'], det_data['detection_boxes'], det_data['detection_scores']):\n",
        "        # Convert to [top-left-x, top-left-y, width, height]\n",
        "        boxf = [float(box[1]), float(box[0]), float(box[3]) - float(box[1]), float(box[2]) - float(box[0])]\n",
        "        scoref = float(score)\n",
        "\n",
        "        # Only add objects with confidence > 0.4\n",
        "        if scoref > 0.4:\n",
        "            detections.append(\n",
        "                fo.Detection(\n",
        "                    label=cls,\n",
        "                    bounding_box= boxf,\n",
        "                    confidence=float(score)\n",
        "                )\n",
        "            )\n",
        "    sample[\"object_faster_rcnn\"] = fo.Detections(detections=detections)\n",
        "    sample.save()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e287dbb4",
      "metadata": {
        "id": "e287dbb4"
      },
      "source": [
        "### Thêm thông tin CLIP embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "58d5d9ca",
      "metadata": {
        "id": "58d5d9ca"
      },
      "outputs": [],
      "source": [
        "all_keyframe = glob(\"dataset\\\\keyframes\\\\*\\\\*.jpg\")\n",
        "video_keyframe_dict = {}\n",
        "all_video = glob('dataset\\\\keyframes\\\\*')\n",
        "all_video = [v.rsplit('\\\\',1)[-1] for v in all_video]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b361f80a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['L01_V001', 'L01_V002', 'L01_V003', 'L01_V004', 'L01_V005', 'L01_V006', 'L01_V007', 'L01_V008', 'L01_V009', 'L01_V010', 'L01_V011', 'L01_V012', 'L01_V013', 'L01_V014', 'L01_V015', 'L01_V016', 'L01_V017', 'L01_V018', 'L01_V019', 'L01_V020', 'L01_V021', 'L01_V022', 'L01_V023', 'L01_V024', 'L01_V025', 'L01_V026', 'L01_V027', 'L01_V028', 'L01_V029', 'L01_V030', 'L01_V031', 'L02_V001', 'L02_V002', 'L02_V003', 'L02_V004', 'L02_V005', 'L02_V006', 'L02_V007', 'L02_V008', 'L02_V009', 'L02_V010', 'L02_V011', 'L02_V012', 'L02_V013', 'L02_V014', 'L02_V015', 'L02_V016', 'L02_V017', 'L02_V018', 'L02_V019', 'L02_V020', 'L02_V021', 'L02_V022', 'L02_V023', 'L02_V024', 'L02_V025', 'L02_V026', 'L02_V027', 'L02_V028', 'L02_V029', 'L02_V030', 'L03_V001', 'L03_V002', 'L03_V003', 'L03_V004', 'L03_V005', 'L03_V006', 'L03_V007', 'L03_V008', 'L03_V009', 'L03_V010', 'L03_V011', 'L03_V012', 'L03_V013', 'L03_V014', 'L03_V015', 'L03_V016', 'L03_V017', 'L03_V018', 'L03_V019', 'L03_V020', 'L03_V021', 'L03_V022', 'L03_V023', 'L03_V024', 'L03_V025', 'L03_V026', 'L03_V027', 'L03_V028', 'L03_V029', 'L03_V030', 'L03_V031', 'L05_V001', 'L05_V002', 'L05_V003', 'L05_V004', 'L05_V005', 'L05_V006', 'L05_V007', 'L05_V008', 'L05_V009', 'L05_V010', 'L05_V011', 'L05_V012', 'L05_V013', 'L05_V014', 'L05_V015', 'L05_V016', 'L05_V017', 'L05_V018', 'L05_V019', 'L05_V020', 'L05_V021', 'L05_V022', 'L05_V023', 'L05_V024', 'L05_V025', 'L05_V026', 'L05_V027', 'L05_V028', 'L06_V001', 'L06_V002', 'L06_V003', 'L06_V004', 'L06_V005', 'L06_V006', 'L06_V007', 'L06_V008', 'L06_V009', 'L06_V010', 'L06_V011', 'L06_V012', 'L06_V013', 'L06_V014', 'L06_V015', 'L06_V016', 'L06_V017', 'L06_V018', 'L06_V019', 'L06_V020', 'L06_V021', 'L06_V022', 'L06_V023', 'L06_V024', 'L06_V025', 'L06_V026', 'L06_V027', 'L06_V028', 'L07_V001', 'L07_V002', 'L07_V003', 'L07_V004', 'L07_V005', 'L07_V006', 'L07_V007', 'L07_V008', 'L07_V009', 'L07_V010', 'L07_V011', 'L07_V012', 'L07_V013', 'L07_V014', 'L07_V015', 'L07_V016', 'L07_V017', 'L07_V018', 'L07_V019', 'L07_V020', 'L07_V021', 'L07_V022', 'L07_V023', 'L07_V024', 'L07_V025', 'L07_V026', 'L07_V027', 'L07_V028', 'L07_V029', 'L07_V030', 'L07_V031', 'L09_V001', 'L09_V002', 'L09_V003', 'L09_V004', 'L09_V005', 'L09_V006', 'L09_V007', 'L09_V008', 'L09_V009', 'L09_V010', 'L09_V011', 'L09_V012', 'L09_V013', 'L09_V014', 'L09_V015', 'L09_V016', 'L09_V017', 'L09_V018', 'L09_V019', 'L09_V020', 'L09_V021', 'L09_V022', 'L09_V023', 'L09_V024', 'L09_V025', 'L09_V026', 'L09_V027', 'L09_V028', 'L09_V029', 'L09_V030', 'L10_V001', 'L10_V002', 'L10_V003', 'L10_V004', 'L10_V005', 'L10_V006', 'L10_V007', 'L10_V008', 'L10_V009', 'L10_V010', 'L10_V011', 'L10_V012', 'L10_V013', 'L10_V014', 'L10_V015', 'L10_V016', 'L10_V017', 'L10_V018', 'L10_V019', 'L10_V020', 'L10_V021', 'L10_V022', 'L10_V023', 'L10_V024', 'L10_V025', 'L10_V026', 'L10_V027', 'L10_V028', 'L10_V029', 'L10_V030']\n"
          ]
        }
      ],
      "source": [
        "print(all_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53460b70",
      "metadata": {
        "id": "53460b70"
      },
      "source": [
        "Đọc thông tin clip embedding được cung cấp.\n",
        "\n",
        "Lưu ý: Các bạn cần tải đúng bản CLIP embedding từ model **CLIP ViT-B/32**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb927629",
      "metadata": {
        "id": "cb927629"
      },
      "source": [
        "Tạo dictionary `video_keyframe_dict` với `video_keyframe_dict[video]` thông tin danh sách `keyframe` của `video`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f33da133",
      "metadata": {
        "id": "f33da133"
      },
      "outputs": [],
      "source": [
        "for kf in all_keyframe:\n",
        "    _, vid, kf = kf[:-4].rsplit('\\\\',2)\n",
        "    if vid not in video_keyframe_dict.keys():\n",
        "        video_keyframe_dict[vid] = [kf]\n",
        "    else:\n",
        "        video_keyframe_dict[vid].append(kf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faefe0bf",
      "metadata": {
        "id": "faefe0bf"
      },
      "source": [
        "Do thông tin vector CLIP embedding được cung cấp được lưu theo từng video nhầm mục đích tối ưu thời gian đọc dữ liệu. Cần sort lại danh sách `keyframe` của từng `video` để đảm bảo thứ tự đọc đúng với vector embedding được cung cấp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4b0fad7a",
      "metadata": {
        "id": "4b0fad7a"
      },
      "outputs": [],
      "source": [
        "for k,v in video_keyframe_dict.items():\n",
        "    video_keyframe_dict[k] = sorted(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "249b0cab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "video_keyframe_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d99d121",
      "metadata": {
        "id": "9d99d121"
      },
      "source": [
        "Tạo dictionary `embedding_dict` với `embedding_dict[video][keyframe]` lưu thông tin vector CLIP embedding của `keyframe` trong `video` tương ứng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a5d94d7b",
      "metadata": {
        "id": "a5d94d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset\\clip-features-vit-b32\\L01_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V029.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V030.npy\n",
            "dataset\\clip-features-vit-b32\\L01_V031.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V029.npy\n",
            "dataset\\clip-features-vit-b32\\L02_V030.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V029.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V030.npy\n",
            "dataset\\clip-features-vit-b32\\L03_V031.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L05_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L06_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V029.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V030.npy\n",
            "dataset\\clip-features-vit-b32\\L07_V031.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V029.npy\n",
            "dataset\\clip-features-vit-b32\\L09_V030.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V001.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V002.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V003.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V004.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V005.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V006.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V007.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V008.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V009.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V010.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V011.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V012.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V013.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V014.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V015.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V016.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V017.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V018.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V019.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V020.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V021.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V022.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V023.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V024.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V025.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V026.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V027.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V028.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V029.npy\n",
            "dataset\\clip-features-vit-b32\\L10_V030.npy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "embedding_dict = {}\n",
        "for v in all_video:\n",
        "    clip_path = f'dataset\\\\clip-features-vit-b32\\\\{v}.npy'\n",
        "    print(clip_path)\n",
        "    a = np.load(clip_path)\n",
        "    embedding_dict[v] = {}\n",
        "    for i,k in enumerate(video_keyframe_dict[v]):\n",
        "        embedding_dict[v][k] = a[i]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea0432e",
      "metadata": {
        "id": "5ea0432e"
      },
      "source": [
        "Tạo danh sách `clip_embedding` ứng với danh sách `sample` trong `dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1ad16b5c",
      "metadata": {
        "id": "1ad16b5c"
      },
      "outputs": [],
      "source": [
        "clip_embeddings = []\n",
        "for sample in dataset:\n",
        "    clip_embedding = embedding_dict[sample['video']][sample['frameid']]\n",
        "    clip_embeddings.append(clip_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "940e2e55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "a = np.load('D:\\WorkSpace\\Contest\\HCM_AIC2023\\dataset\\clip_embeddings.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f217e56c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Sample: {\n",
            "    'id': '65017bf58c34b7c265c46da7',\n",
            "    'media_type': 'image',\n",
            "    'filepath': 'D:\\\\WorkSpace\\\\Contest\\\\HCM_AIC2023\\\\dataset\\\\keyframes\\\\L01_V001\\\\0001.jpg',\n",
            "    'tags': [],\n",
            "    'metadata': None,\n",
            "    'video': 'L01_V001',\n",
            "    'frameid': '0001',\n",
            "}>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n",
            "\n",
            "Could not connect session, trying again in 10 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sample in dataset:\n",
        "    print(sample)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a1eb5e96",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(67619, 512)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(clip_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f791d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"clip_embeddings.npy\", clip_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60fd7728",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "lst = []\n",
        "for sample in dataset:\n",
        "    lst.append([sample[\"filepath\"], sample[\"video\"], sample[\"frameid\"]])\n",
        "\n",
        "df = pd.DataFrame(lst, columns = [\"filepath\", \"video\", \"frameid\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655d2feb",
      "metadata": {
        "id": "655d2feb"
      },
      "outputs": [],
      "source": [
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    model=\"clip-vit-base32-torch\",      # store model's name for future use\n",
        "    embeddings=clip_embeddings,          # precomputed image embeddings\n",
        "    brain_key=\"img_sim\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3136339f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_frame_feature_vector(video, frameids):\n",
        "    image_ids = pd.read_csv(\"dataset/image_ids.csv\", dtype={\"filepath\": \"string\", \"video\": \"string\", \"frameid\": \"string\"})\n",
        "    image_ids = list(zip(image_ids['video'], image_ids[\"frameid\"]))\n",
        "    \n",
        "    img_idx = image_ids.index((video, frameids))\n",
        "\n",
        "    image_clipfeatures = np.load(\"dataset/clip_embeddings.npy\")\n",
        "\n",
        "    photo_feature = image_clipfeatures[img_idx].astype(np.float32)\n",
        "\n",
        "    photo_feature = np.expand_dims(photo_feature, axis=0)\n",
        "    return photo_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2eea3720",
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "import clip\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "88c22dc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12fc908a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_index_vector(photo_features):\n",
        "    index = faiss.IndexFlatL2(512)\n",
        "    fe = photo_features.reshape(photo_features.shape[0], -1).astype('float32')\n",
        "    print(np.shape(fe))\n",
        "    index.add(fe)\n",
        "    faiss.write_index(index, 'faiss_index.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "22b2a0b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(67619, 512)\n"
          ]
        }
      ],
      "source": [
        "image_features = np.load(\"dataset/clip_embeddings.npy\")\n",
        "create_index_vector(image_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "276343f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_feature_vector(text_query):\n",
        "    text = clip.tokenize([text_query]).to(device)  \n",
        "    text_features = model.encode_text(text).cpu().detach().numpy().astype(np.float32)\n",
        "    print(text_features.shape)\n",
        "    return text_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "4b16b0ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_vector(query, faiss_index, topk):\n",
        "    features_vector_search = get_feature_vector(query)\n",
        "    f_dist, f_ids = faiss_index.search(features_vector_search, topk)\n",
        "    faiss_index.reset()\n",
        "    f_dist = np.array(f_dist[0])\n",
        "    sorted_indices = np.argsort(-1 * f_dist)\n",
        "    f_ids = np.array(f_ids[0])[sorted_indices]\n",
        "    return f_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "06ff5e40",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 512)\n"
          ]
        }
      ],
      "source": [
        "query = \"There is a dog in a picture\"\n",
        "faiss_index = faiss.read_index(\"dataset/faiss_index.bin\")\n",
        "topk = 5\n",
        "ids = search_vector(query, faiss_index, topk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "0f1bf5cc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([36384, 22259, 12853, 36385, 36391], dtype=int64)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "80b1d41f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "image_ids = pd.read_csv(\"dataset/image_ids.csv\", dtype={\"filepath\": \"string\", \"video\": \"string\", \"frameid\": \"string\"})\n",
        "image_ids = list(zip(image_ids['video'], image_ids[\"frameid\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e53bdc37",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('L06_V011', '0088')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(image_ids[36391])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99223338",
      "metadata": {
        "id": "99223338"
      },
      "source": [
        "## Từ đây các bạn có thể thử các tính năng search, filter trên ứng dụng fiftyone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46605386",
      "metadata": {
        "id": "46605386"
      },
      "outputs": [],
      "source": [
        "# Bạn cần phải cài version umap-learn hỗ trợ.\n",
        "# fob.compute_visualization(\n",
        "#     dataset,\n",
        "#     embeddings=clip_embeddings,\n",
        "#     brain_key=\"img_viz\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381972d8",
      "metadata": {
        "id": "381972d8"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
